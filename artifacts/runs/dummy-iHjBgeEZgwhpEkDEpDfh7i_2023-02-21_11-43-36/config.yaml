PPO args:
- learning_rate
- n_steps
- batch_size
- n_epochs
- gamma
- gae_lambda
- clip_range
- clip_range_vf
- normalize_advantage
- ent_coef
- vf_coef
- max_grad_norm
- use_sde
- sde_sample_freq
- target_kl
- tensorboard_log
- verbose
- seed
- device
- _init_setup_model
algorithm: PPO
artifact_dir: artifacts/runs/dummy-iHjBgeEZgwhpEkDEpDfh7i_2023-02-21_11-43-36/
batch_size: 256
checkpoint_freq: 655360
clip_range: 0.165
clip_range_vf: null
device: auto
ent_coef: 0.009
env_config_passthrough:
- lam
- b
- theta
- mu
- delta_t
- inspection_cost
- repair_cost
- repair_effect
- replacement_cost
- failure_cost
- failure_threshold
- initial_state
- time_horizon
- steps_per_observation
env_id: HieracicalGammaProcessSystem2
env_kwargs:
  b: 2
  delta_t: 0.1
  failure_cost: 1000
  failure_threshold: 25
  initial_state: 0.0
  inspection_cost: 5
  lam: 10
  mu: -6.326179874079297
  repair_cost: 50
  repair_effect: 0.5
  replacement_cost: 500
  steps_per_observation: 1
  theta: 0.22314355131420976
  time_horizon: 200
env_params:
- env_id
- env_kwargs
eval_episodes: 32
eval_freq: 65536
eval_seed: 4096
gae_lambda: 0.91
gamma: 1.0
learning_rate: 5.0e-05
max_grad_norm: 0.56
model_device: auto
n_epochs: 10
n_steps: 2048
no_logging: true
normalize_advantage: true
num_envs: 32
policy_net_arch_l1: 80
policy_net_arch_l2: 57
policy_net_arch_l3: 0
run_name: dummy-iHjBgeEZgwhpEkDEpDfh7i
sde_sample_freq: -1
seed: null
sweep: false
target_kl: null
tensorboard_log: null
timesteps: 25000000
use_sde: false
vec_normalize: true
vec_normalize_kwargs:
  norm_obs: true
  norm_reward: true
  training: true
vec_normalize_load_path: null
verbose: 2
vf_coef: 0.639
